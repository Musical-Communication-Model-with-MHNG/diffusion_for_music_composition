#------environment of model-------

#------parameter of DDPM-------
beta: #β
  start: 0.0001
  end: 0.02
p_cond: 0.5 #probability of condition

#------setting text encoder-------
text_encoder: CLIP #type of text encoder
CLIP:
  pre_trained_model: ViT-B/32 #pre-trained model of CLIP
  parameter_path: 
  
#------ setting U-Net -------
u_net:
  channel_init: 2 # default: 2  polyffusionは2で実験済み
  channel: 64 # default: 64
  num_blocks: 2 # default: 2
  multiple_layer: [1, 2, 4, 4] # default: [1, 2, 2, 4] polyffusionは[1,2,4,4]で実験済み
  use_attention: [False, False, False, True] # default: [False, False, False, True]
  attention_levels: [2, 3] #default: [2, 3]
  n_head: 4 # default: 4
  tf_layer: 1 # default: 1
  d_cond: 512 # default: 512

#------ setting ProbVLM -------
ProbVLM:
  inp_dim: 512 #input dimension of ProbVLM
  out_dim: 512 #output dimension of ProbVLM
  hid_dim: 512 #hidden dimension of ProbVLM
  num_layers: 3 #number of layers of ProbVLM
  p_drop: 0.05 #dropout rate of ProbVLM
  parameter_path: 
  use_lora: False #flag of using LoRa
  lora:
    r: 8 #rank
    lora_alpha: 16 #alpha
    target_modules: ['0', '2', '5']
    lora_dropout: 0.05
    bias: none